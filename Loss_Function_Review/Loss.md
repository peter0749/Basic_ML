### Categorical Cross-Entropy

輸入：兩個離散機率分佈 (y, p)

其中 `y` 代表預測出來的機率分佈， `p` 代表實際的機率分佈。

要注意因為是機率分佈，機率分佈的總和必須 == 1 ，

否則會發生錯誤。

```
L(theta) = 1/n * sum_{i=1->n}{y_i*log(p_i) + (1-y_i)*log(1-p_i)}
```

以上的公式可以理解成：

假設 p_i==1 p_j==0, j!=i ，只有下標 `i` 的直是正確的類別，

那麼取 log 後， `y_i*log(p_i)` 項會是 0 （代表預測正確，沒有失誤），
而其餘 p_j 部分的像也都會是 0 ，因為 `sum_{j=1->n, j!=i}{p_j}==0` ，
整個 Loss function 會輸出 0 ，代表預測完全正確。

反之，如果 `p_i<1` ， `sum_{j=1->n,j!=i}{p_j}>0` ，

會造成 `log(p_i) > 0` ，而某些 `p_j*log(1-p_j) > 0` ，

因此 Loss 會大於 0 ，可以想像成我們要同時懲罰應該被分類到而未被分類的，
還有不應該被分類到而被分類的，上述公式又是可微分的函數，所以可以利用梯度下降使得最後 `L*(theta) -> 0`
