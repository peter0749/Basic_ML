### Categorical Cross-Entropy

輸入：兩個離散機率分佈 (y, p)

其中 `y` 代表預測出來的機率分佈， `p` 代表實際的機率分佈。

要注意因為是機率分佈，機率分佈的總和必須 == 1 ，
每個類別的機率值也必須 > 0 ，

否則會發生錯誤。

```
L(theta) = 1/n * sum_{i=1->n}{y_i*log(p_i) + (1-y_i)*log(1-p_i)}
```

以上的公式可以理解成：

假設 p_i==1 p_j==0, j!=i ，只有下標 `i` 的直是正確的類別，

那麼取 log 後， `y_i*log(p_i)` 項會是 0 （代表預測正確，沒有失誤），
而其餘 p_j 部分的像也都會是 0 ，因為 `sum_{j=1->n, j!=i}{p_j}==0` ，
整個 Loss function 會輸出 0 ，代表預測完全正確。

反之，如果 `p_i<1` ， `sum_{j=1->n,j!=i}{p_j}>0` ，

會造成 `log(p_i) > 0` ，而某些 `p_j*log(1-p_j) > 0` ，

因此 Loss 會大於 0 ，可以想像成我們要同時懲罰應該被分類到而未被分類的，
還有不應該被分類到而被分類的，上述公式又是可微分的函數，所以可以利用梯度下降使得最後 `L*(theta) -> 0`

Loss 的正負號實作時會有差異，但不影響整體概念。


要使用這個 Loss Function ，可以在我們的 model 最後加上一層 `softmax` 函數，會將輸出的 score 壓成一個離散的機率分佈，就可以用這個 Loss function 優化整個 model。

另外，當類別數等於 2 時，就是一個 `Binomial cross-entropy loss` ，也可用於二元分類問題。

